{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P0q2aRt4m6lK"
   },
   "source": [
    "# Traffic Sign Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpjM7QLhm5NP"
   },
   "outputs": [],
   "source": [
    "# Unzip the uploaded data in google drive\n",
    "# ! unzip \"/content/drive/My Drive/Traffic Sign Recognition/traffic sign recognition.zip\" -d \"/content/drive/My Drive/Traffic Sign Recognition\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTHSCEf065vo"
   },
   "source": [
    "## Get our workspace ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HfIHkcDknZX6"
   },
   "outputs": [],
   "source": [
    "# # Import tensorflow into colab\n",
    "# try:\n",
    "#   %tensorflow_version 2.x\n",
    "# except Exception:\n",
    "#   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "pntheEyJ7BNY",
    "outputId": "4c2b22ba-15df-4c38-f155-0637ef0ed1d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version:  2.0.0\n"
     ]
    }
   ],
   "source": [
    "# importing necessary tools\n",
    "import tensorflow as tf\n",
    "print(\"TF version: \", tf.__version__)\n",
    "\n",
    "# Check for GPU availability\n",
    "# print(\"GPU\", \"Available (yes)\" if tf.config.list_physical_devices(\"GPU\") else \"Not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1WFTurpiAy2y"
   },
   "source": [
    "## Getting our data ready (turning into tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "xBpAXlRN7MAf",
    "outputId": "1d09462d-caa0-4e22-f940-ee6abf433047"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Width</th>\n",
       "      <th>Height</th>\n",
       "      <th>Roi.X1</th>\n",
       "      <th>Roi.Y1</th>\n",
       "      <th>Roi.X2</th>\n",
       "      <th>Roi.Y2</th>\n",
       "      <th>ClassId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "      <td>39209.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.835880</td>\n",
       "      <td>50.328930</td>\n",
       "      <td>5.999515</td>\n",
       "      <td>5.962381</td>\n",
       "      <td>45.197302</td>\n",
       "      <td>44.728379</td>\n",
       "      <td>15.788390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>24.306933</td>\n",
       "      <td>23.115423</td>\n",
       "      <td>1.475493</td>\n",
       "      <td>1.385440</td>\n",
       "      <td>23.060157</td>\n",
       "      <td>21.971145</td>\n",
       "      <td>12.013238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>35.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>43.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>58.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>243.000000</td>\n",
       "      <td>225.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>223.000000</td>\n",
       "      <td>205.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Width        Height        Roi.X1        Roi.Y1        Roi.X2  \\\n",
       "count  39209.000000  39209.000000  39209.000000  39209.000000  39209.000000   \n",
       "mean      50.835880     50.328930      5.999515      5.962381     45.197302   \n",
       "std       24.306933     23.115423      1.475493      1.385440     23.060157   \n",
       "min       25.000000     25.000000      0.000000      5.000000     20.000000   \n",
       "25%       35.000000     35.000000      5.000000      5.000000     29.000000   \n",
       "50%       43.000000     43.000000      6.000000      6.000000     38.000000   \n",
       "75%       58.000000     58.000000      6.000000      6.000000     53.000000   \n",
       "max      243.000000    225.000000     20.000000     20.000000    223.000000   \n",
       "\n",
       "             Roi.Y2       ClassId  \n",
       "count  39209.000000  39209.000000  \n",
       "mean      44.728379     15.788390  \n",
       "std       21.971145     12.013238  \n",
       "min       20.000000      0.000000  \n",
       "25%       30.000000      5.000000  \n",
       "50%       38.000000     12.000000  \n",
       "75%       52.000000     25.000000  \n",
       "max      205.000000     42.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv('./Data/Train.csv')\n",
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "y0c32l-n70WB",
    "outputId": "542d3fb1-c4de-4efe-8094-39811ccbbf47"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClassId</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00003.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20</td>\n",
       "      <td>Train/20/00020_00000_00004.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ClassId                            Path\n",
       "0       20  Train/20/00020_00000_00000.png\n",
       "1       20  Train/20/00020_00000_00001.png\n",
       "2       20  Train/20/00020_00000_00002.png\n",
       "3       20  Train/20/00020_00000_00003.png\n",
       "4       20  Train/20/00020_00000_00004.png"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = train_df.drop(['Width', 'Height', 'Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2'], axis = 1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "colab_type": "code",
    "id": "CPR-POJT9IgE",
    "outputId": "f4dce800-6355-4024-ab1d-18a11183b89f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df['ClassId'].value_counts().plot.bar(figsize=(20, 10))\n",
    "train_df['ClassId'].value_counts().median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j6vaHN8LBA1p"
   },
   "source": [
    "### Getting images and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "INj3nRAx_G-n",
    "outputId": "2ba08c5d-a675-417b-ec23-6242b3bdac11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/Machine Learning/Traffic Sign Recognition/Data/Train/20/00020_00000_00000.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Train/20/00020_00000_00001.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Train/20/00020_00000_00002.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Train/20/00020_00000_00003.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Train/20/00020_00000_00004.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Train/20/00020_00000_00005.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Train/20/00020_00000_00006.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Train/20/00020_00000_00007.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Train/20/00020_00000_00008.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Train/20/00020_00000_00009.png']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create pathnames from image Id's\n",
    "filenames = ['D:/Machine Learning/Traffic Sign Recognition/Data/' + fname for fname in train_df['Path']]\n",
    "filenames[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ddaW38TSBao6",
    "outputId": "948eed7c-9afd-439a-c9c4-af66aec0669f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 20, 20, ..., 42, 42, 42], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "labels = train_df['ClassId'].to_numpy()\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RMiMLzu3Bv20",
    "outputId": "2742c6be-e62e-4e65-e6ab-d49b15244754"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39209"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "hVh-Y-OOBxgF",
    "outputId": "7568a29b-2a17-479e-c847-af99ab37f127"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_signs = np.unique(labels)\n",
    "len(unique_signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "KG5xU1ePB9DT",
    "outputId": "b45f1b30-f387-420f-b3e7-34f114bc2f83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the labels into one hot encoding\n",
    "labels = tf.keras.utils.to_categorical(labels, 43)\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4s4QuPG-ESAt",
    "outputId": "2b90d441-5790-43a1-b183-ec0b546df5c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39209"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6yWCARg8EvAg"
   },
   "source": [
    "### Creating Validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Z33IiqbkErad",
    "outputId": "654139a2-1297-4cde-9095-ab4883708f54"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31367, 31367, 7842, 7842)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create X & y variables\n",
    "X = filenames\n",
    "y = labels\n",
    "\n",
    "# Splitting our data into train and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "len(X_train), len(y_train), len(X_val), len(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rr5GS74PfE0E"
   },
   "source": [
    "### Processing image and turning into Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbtBuZ60Fqff"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "\n",
    "def process_image(image_path):\n",
    "    \"\"\"\n",
    "    Takes an image file path and turns the image into a Tensor.\n",
    "    \"\"\"\n",
    "    # Read in an image file\n",
    "    image = tf.io.read_file(image_path)\n",
    "    # Turn the jpeg image into numerical Tensor with 3 colour channels (Red, Green, Blue)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    # Convert the colour channel values from 0-255 to 0-1 values\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # Resize the image to our desired value (32, 32)\n",
    "    image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QMA223Hffwa5"
   },
   "source": [
    "### Turning data into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qHxqQQT7ftzf"
   },
   "outputs": [],
   "source": [
    "# Create a simple function to return tuple\n",
    "\n",
    "def get_image_label (image_path, label):\n",
    "    \"\"\"\n",
    "    Takes an image file path name and the assosciated label,\n",
    "    processes the image and reutrns a typle of (image, label).\n",
    "    \"\"\"\n",
    "    image = process_image(image_path)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "igbbrbGGgCjn"
   },
   "outputs": [],
   "source": [
    "# Define batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Create a function to turn data into batches\n",
    "def create_data_batches (X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):\n",
    "    \"\"\"\n",
    "    Creates batches of data out of image (X) and label (y) pairs.\n",
    "    Shuffles the data if it's training data but doesn't shuffle if it's validation data.\n",
    "    Also accepts test data as input (no labels).\n",
    "    \"\"\"\n",
    "    # If the data is a test dataset, we probably don't have have labels\n",
    "    if test_data:\n",
    "        print(\"Creating test data batches...\")\n",
    "        data = tf.data.Dataset.from_tensor_slices((tf.constant(X)))\n",
    "        data_batch = data.map(process_image).batch(BATCH_SIZE)\n",
    "        # If the data is a valid dataset, we don't need to shuffle it\n",
    "    elif valid_data:\n",
    "        print(\"Creating validation dataset batches...\")\n",
    "        data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y)))\n",
    "        # Create (image, label) tuples (this also turns the iamge path into a preprocessed image)\n",
    "        data_batch = data.map(get_image_label).batch(BATCH_SIZE)\n",
    "    else:\n",
    "        print(\"Creating training dataset batches...\")\n",
    "        # Turn filepaths and labels into Tensors\n",
    "        data = tf.data.Dataset.from_tensor_slices((tf.constant(X), tf.constant(y)))\n",
    "        # Shuffling pathnames and labels before mapping image processor function is faster than shuffling images\n",
    "        data = data.shuffle(buffer_size=len(X))\n",
    "        # Create (image, label) tuples (this also turns the iamge path into a preprocessed image) and turning into batches\n",
    "        data_batch = data.map(get_image_label).batch(BATCH_SIZE)\n",
    "    return data_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "0DR7avHtgh0_",
    "outputId": "aec695dd-5589-4735-93e0-9cd9dce57f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset batches...\n",
      "Creating validation dataset batches...\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation batches\n",
    "train_data = create_data_batches(X_train, y_train)\n",
    "val_data = create_data_batches(X_val, y_val, valid_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "6_ShflIEgpFj",
    "outputId": "6f856215-5d46-4fd7-9a0c-1122dabb0423"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(None, 43), dtype=tf.float32, name=None)),\n",
       " (TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name=None),\n",
       "  TensorSpec(shape=(None, 43), dtype=tf.float32, name=None)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out the different attributes of our data batches\n",
    "train_data.element_spec, val_data.element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W6YAKgQnhBRN"
   },
   "source": [
    "### Visualizing Data Batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UTYd0M2XgwAC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a function for viewing images in a data batch\n",
    "def show_25_images (images, labels):\n",
    "    \"\"\"\n",
    "    Displays a plot of 25 images and their labels from a data batch.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(25):\n",
    "        ax = plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(unique_signs[labels[i].argmax()])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8PuBM4tqhgpY"
   },
   "outputs": [],
   "source": [
    "# # Visualizing traing batch\n",
    "# train_images, train_labels = next(train_data.as_numpy_iterator())\n",
    "# show_25_images(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oCTnfDK7jvyi"
   },
   "outputs": [],
   "source": [
    "# # Visualizing validation data\n",
    "# val_images, val_labels = next(val_data.as_numpy_iterator())\n",
    "# show_25_images(val_images, val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k32Yn2Mak4ts"
   },
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjnAVn3Vjx0i"
   },
   "outputs": [],
   "source": [
    "# Setup input shape to the model\n",
    "INPUT_SHAPE = [IMG_SIZE, IMG_SIZE, 3]\n",
    "\n",
    "# SEtup the output shape\n",
    "OUTPUT_SHAPE = len(unique_signs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z2tSHj9vlFzw"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries for creating model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense, Conv2D, MaxPool2D\n",
    "\n",
    "# Creating CNN Model\n",
    "def traffic_sign_net(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(rate=0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(43, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tr8D0a4Vluhl"
   },
   "outputs": [],
   "source": [
    "# Create a function that creates model\n",
    "def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE):\n",
    "    # Setup the model layers\n",
    "    model = traffic_sign_net(input_shape=input_shape)\n",
    "\n",
    "    # Compile the model\n",
    "    print(\"Compiling the model\")\n",
    "    model.compile(\n",
    "      loss = tf.keras.losses.CategoricalCrossentropy(),\n",
    "      optimizer = tf.keras.optimizers.Adam(),\n",
    "      metrics = [\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PVEXppesYHV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        2432      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 24, 24, 32)        25632     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 43)                11051     \n",
      "=================================================================\n",
      "Total params: 356,939\n",
      "Trainable params: 356,939\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1wPrihC1tBAU"
   },
   "source": [
    "## Creating callbacks\n",
    "\n",
    "###Creating Tensorboard callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S8PFmCGatIf6"
   },
   "outputs": [],
   "source": [
    "# LoadTensorboard notebook extension\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JukkOaBetT50"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "# Function to build a TensorBoard callback\n",
    "def create_tensorboard_callback():\n",
    "    logdir = os.path.join(\"D:\\Machine Learning\\Traffic Sign Recognition\\logs\",\n",
    "                        datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "                        )\n",
    "    # Make it so the logs get tracked whenever we run an experiment\n",
    "    return tf.keras.callbacks.TensorBoard(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E3HMb-IctrSH"
   },
   "source": [
    "### Creating Early Stopping callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6f4IRhbtp7N"
   },
   "outputs": [],
   "source": [
    "# Create early stopping callback\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jBrChATXDoCT"
   },
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7mN8EyvD29P"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xNqkky8uD8b7"
   },
   "outputs": [],
   "source": [
    "# Build a fn to train and return a trained model\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    Trains a given model and returns the trained version.\n",
    "    \"\"\"\n",
    "    # Create a model\n",
    "    model = create_model()\n",
    "    # Create new TensorBoard session everytime we train a model\n",
    "    tensorboard = create_tensorboard_callback()\n",
    "    # Fit the model to the data passing it the callbacks we created\n",
    "    model.fit(x=train_data,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            validation_data=val_data,\n",
    "            validation_freq=1,\n",
    "            callbacks=[tensorboard, early_stopping])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "tD5qyJimEBsz",
    "outputId": "2ff4dea0-7507-46dc-f6b4-614c8c45d427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model\n",
      "Epoch 1/30\n",
      "491/491 [==============================] - 559s 1s/step - loss: 1.4984 - accuracy: 0.5792 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/30\n",
      "491/491 [==============================] - 124s 253ms/step - loss: 0.2690 - accuracy: 0.9186 - val_loss: 0.0588 - val_accuracy: 0.9833\n",
      "Epoch 3/30\n",
      "491/491 [==============================] - 128s 260ms/step - loss: 0.1526 - accuracy: 0.9543 - val_loss: 0.0406 - val_accuracy: 0.9901\n",
      "Epoch 4/30\n",
      "491/491 [==============================] - 128s 260ms/step - loss: 0.1095 - accuracy: 0.9674 - val_loss: 0.0373 - val_accuracy: 0.9880\n",
      "Epoch 5/30\n",
      "491/491 [==============================] - 128s 262ms/step - loss: 0.0915 - accuracy: 0.9721 - val_loss: 0.0253 - val_accuracy: 0.9934\n",
      "Epoch 6/30\n",
      "491/491 [==============================] - 128s 260ms/step - loss: 0.0716 - accuracy: 0.9786 - val_loss: 0.0211 - val_accuracy: 0.9954\n",
      "Epoch 7/30\n",
      "491/491 [==============================] - 127s 259ms/step - loss: 0.0713 - accuracy: 0.9793 - val_loss: 0.0224 - val_accuracy: 0.9946\n",
      "Epoch 8/30\n",
      "491/491 [==============================] - 130s 265ms/step - loss: 0.0589 - accuracy: 0.9821 - val_loss: 0.0192 - val_accuracy: 0.9941\n",
      "Epoch 9/30\n",
      "491/491 [==============================] - 129s 262ms/step - loss: 0.0538 - accuracy: 0.9825 - val_loss: 0.0158 - val_accuracy: 0.9959\n",
      "Epoch 10/30\n",
      "491/491 [==============================] - 128s 260ms/step - loss: 0.0493 - accuracy: 0.9847 - val_loss: 0.0179 - val_accuracy: 0.9954\n",
      "Epoch 11/30\n",
      "491/491 [==============================] - 129s 262ms/step - loss: 0.0453 - accuracy: 0.9867 - val_loss: 0.0181 - val_accuracy: 0.9954\n",
      "Epoch 12/30\n",
      "491/491 [==============================] - 129s 263ms/step - loss: 0.0458 - accuracy: 0.9860 - val_loss: 0.0206 - val_accuracy: 0.9943\n"
     ]
    }
   ],
   "source": [
    "# Fit the model to data\n",
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir D:/Machine\\ Learning/Traffic\\ Sign\\ Recognition/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H33Dp8za4YuI"
   },
   "outputs": [],
   "source": [
    "# Function for saving the model\n",
    "def save_model(model, suffix=None):\n",
    "    \"\"\"\n",
    "    Saves a given model in a models directory and appends a suffix (string).\n",
    "    \"\"\"\n",
    "    # Create a model directory pathname with current time\n",
    "    modeldir = os.path.join(\"D:\\Machine Learning\\Traffic Sign Recognition\\models\",\n",
    "                            datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    if suffix == None:\n",
    "        model_path = modeldir + \".h5\"\n",
    "    else:\n",
    "        model_path = modeldir + \"-\" + suffix + \".h5\" # save format of model\n",
    "    print(f\"Saving model to: {model_path}...\")\n",
    "    model.save(model_path)\n",
    "    return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to load a trained model\n",
    "def load_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads a saved model from a specified path.\n",
    "    \"\"\"\n",
    "    print(f\"Loading model from: {model_path}\")\n",
    "    model =tf.keras.models.load_model(model_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0I830pVG5BFh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to: D:\\Machine Learning\\Traffic Sign Recognition\\models\\20200415-215022.h5...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'D:\\\\Machine Learning\\\\Traffic Sign Recognition\\\\models\\\\20200415-215022.h5'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: D:\\Machine Learning\\Traffic Sign Recognition\\models\\20200415-215022.h5\n"
     ]
    }
   ],
   "source": [
    "model = load_model('D:\\\\Machine Learning\\\\Traffic Sign Recognition\\\\models\\\\20200415-215022.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test dataset batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClassId</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>Test/00000.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Test/00001.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Test/00002.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>Test/00003.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>Test/00004.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ClassId            Path\n",
       "0       16  Test/00000.png\n",
       "1        1  Test/00001.png\n",
       "2       38  Test/00002.png\n",
       "3       33  Test/00003.png\n",
       "4       11  Test/00004.png"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('./Data/test.csv')\n",
    "test_df = test_df.drop(['Width', 'Height', 'Roi.X1', 'Roi.Y1', 'Roi.X2', 'Roi.Y2'], axis=1)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:/Machine Learning/Traffic Sign Recognition/Data/Test/00000.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Test/00001.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Test/00002.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Test/00003.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Test/00004.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Test/00005.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Test/00006.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Test/00007.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Test/00008.png',\n",
       " 'D:/Machine Learning/Traffic Sign Recognition/Data/Test/00009.png']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_img_paths = ['D:/Machine Learning/Traffic Sign Recognition/Data/' + path for path in test_df['Path']]\n",
    "test_img_paths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data batches...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[16, 1, 38, 33, 11, 38, 18, 12, 25, 35]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = create_data_batches(test_img_paths, test_data=True)\n",
    "y_test = list(test_df['ClassId'])\n",
    "y_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making and Evaluating predictions using a trained model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198/198 [==============================] - 19s 97ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.7278415e-19, 6.8732826e-17, 4.4170265e-21, 1.3421904e-17,\n",
       "        5.3345865e-19, 3.3354544e-15, 6.3675620e-20, 1.5916060e-17,\n",
       "        5.5355142e-17, 2.9443739e-09, 7.7476118e-18, 3.4468212e-17,\n",
       "        5.8704669e-14, 4.8020187e-19, 1.0878617e-21, 2.1036480e-12,\n",
       "        1.0000000e+00, 3.9577751e-24, 4.2025268e-19, 1.0486517e-21,\n",
       "        1.5775754e-18, 3.3627058e-22, 1.6319566e-24, 2.9216755e-18,\n",
       "        1.9665689e-19, 4.0455018e-23, 6.1228364e-18, 5.0811746e-21,\n",
       "        1.1812345e-17, 3.7126360e-20, 6.5109463e-24, 8.7859260e-26,\n",
       "        1.8590894e-11, 3.7875677e-19, 1.9326858e-19, 1.9152013e-14,\n",
       "        5.7124195e-20, 1.4250071e-20, 9.5570337e-17, 1.9700594e-20,\n",
       "        1.9538742e-13, 4.9216442e-16, 1.6796406e-18],\n",
       "       [1.5331840e-11, 9.9999928e-01, 3.5619523e-15, 3.1865575e-25,\n",
       "        1.6445321e-13, 1.0567962e-11, 7.4464378e-07, 1.7748678e-15,\n",
       "        9.3357783e-25, 1.6618541e-30, 1.3025229e-25, 1.0955031e-20,\n",
       "        1.3789786e-20, 3.0786530e-22, 1.2721566e-24, 1.0177851e-19,\n",
       "        3.2135614e-24, 2.2147097e-19, 5.1914097e-17, 9.7422515e-32,\n",
       "        4.1645301e-17, 4.7137042e-25, 2.6756149e-31, 4.1889800e-28,\n",
       "        9.9740078e-24, 3.7118849e-19, 9.3701427e-24, 1.3420194e-26,\n",
       "        3.9441045e-23, 7.7925510e-23, 3.3208240e-24, 1.0771657e-21,\n",
       "        1.9011276e-16, 2.2836267e-19, 1.5902086e-25, 4.5644727e-21,\n",
       "        6.6124078e-26, 2.9676998e-22, 7.2327887e-24, 1.9058270e-22,\n",
       "        5.4255219e-18, 1.7255191e-22, 3.9696731e-16]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert probabilities to labels\n",
    "def get_pred_label(prediction_probabilities):\n",
    "  \"\"\"\n",
    "  Turns an array of prediction probabilities into a label.\n",
    "  \"\"\"\n",
    "  return unique_signs[np.argmax(prediction_probabilities)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 1, 38, 33, 11, 38, 18, 12, 25, 35]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turning probabilities to labels\n",
    "pred_labels = []\n",
    "for i in predictions:\n",
    "    pred_labels.append(get_pred_label(i))\n",
    "    \n",
    "pred_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9699129057798892"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the accuracy of the model on test data\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, pred_labels)\n",
    "acc"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "traffic_sign_recognition.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
